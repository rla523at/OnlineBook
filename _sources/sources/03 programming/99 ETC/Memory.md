# 버스
버스(bus)는 컴퓨터 내부에서 여러 구성요소가 **주소, 데이터, 제어 신호**를 주고받기 위해 공유하는 **신호선들의 집합과 그 위에서 동작하는 통신 규약**을 말한다. 메모리 구조 관점에서 버스는 CPU/SoC 내부 인터커넥트 → 메모리 컨트롤러 → DRAM(또는 VRAM) 사이에서 데이터를 실어 나르는 “길”과 “약속(프로토콜)”의 합이다.

## 버스의 기본 3요소

* 주소 버스(Address Bus): 읽고/쓸 메모리의 주소를 전달한다. 폭이 넓을수록(예: 36, 48비트…) 주소 지정 가능한 공간이 커진다.
* 데이터 버스(Data Bus): 실제 데이터 비트들이 이동한다. 폭(예: 64비트, 128비트)이 클수록 한 번에 이동하는 데이터량이 많다.
* 제어 버스(Control Bus): 읽기/쓰기, 유효 신호, 타이밍, 오류 정정 등 동작을 제어한다. (예: RAS#, CAS#, WE#, CS#, CLK, DQS, ECC/Parity 등)

현대 DRAM에서는 주소/명령선과 데이터선(DQ)이 분리되고, 데이터선에는 **DQS(데이터 스토브)** 같은 소스-동기 신호가 함께 다닌다.

---

## 메모리 버스와 관련된 핵심 용어

* 버스 폭(Width): 한 번에 병렬로 전달되는 비트 수. 예: 일반 PC의 DRAM 채널은 64비트, ECC 사용 시 72비트(64+8).
* 전송 레이트(Data Rate): 핀/비트당 초당 전송 수. DDR 계열은 상승/하강 에지 모두 사용(“Double Data Rate”).
* 대역폭(Bandwidth): 이론적 최대 전달량.

  * 대략식: **대역폭[GB/s] = (핀당 전송률[MT/s] × 버스 폭[bit]) / 8**
  * 예시: DDR5-5600, 64비트 채널 → 5600 × 64 / 8 = **44.8 GB/s** (채널 하나 기준)
  * 듀얼 채널이면 약 89.6 GB/s (오버헤드·효율 고려 전 이론치)
* 채널(Channel): 메모리 컨트롤러가 병렬로 다루는 독립 버스 집합. 채널 수가 늘면 총 대역폭이 선형에 가깝게 증가한다.
* 버스트(Burst) 전송: DRAM은 연속 워드들을 묶음으로 전송해 효율을 높인다(버스트 길이, 프리패치 깊이 개념).
* 효율(Effective Throughput): 이론 대역폭에서 프로토콜 오버헤드, 페이지 미스/뱅크 충돌, 지연 때문에 실제 성능은 낮아진다.

---

## 버스 토폴로지와 진화

* 공유 병렬 버스(과거): 여러 장치가 한 “틀”을 공유. 간단하지만 속도·확장성이 제한되고 **중재(Arbitration)** 필요.
* 포인트-투-포인트(Link): 고속에서 일반적. 각 채널/링크가 전기적 부하가 적어 신호 품질과 속도가 좋아진다. 최신 DDR 채널, PCIe 등이 여기에 해당.
* 시스템 인터커넥트: 과거의 FSB(Front-Side Bus)는 인텔 QPI/UPI, AMD HyperTransport/Infinity Fabric 같은 포인트-투-포인트 인터커넥트로 대체. CPU 내부는 링/메시/크로스바 구조를 사용한다. SoC에서는 AMBA AXI/AHB/APB 같은 버스/인터커넥트 프로토콜이 표준.

---

## 메모리 컨트롤러와 스케줄링

메모리 컨트롤러(IMC)는 들어오는 요청을 **뱅크/행/열 구조**에 맞춰 스케줄한다.

* 페이지 히트 우선(예: FR-FCFS)으로 대역폭을 높이고 지연을 줄인다.
* 채널/랭크/뱅크 인터리빙으로 병렬성 확대.
* ECC/Parity로 데이터 오류를 검출·정정.

버스 중재는 공유 버스에서 “누가 말할지”를 결정하는 절차다(중앙 집중식, 분산식, 우선순위/라운드로빈/TDMA 등). 메모리 측면에서는 컨트롤러가 사실상 “마스터”라 별도의 외부 중재가 줄었고, 대신 **요청 스케줄링**의 비중이 커졌다.

---

## 동기/비동기와 타이밍

* 동기식(Synchronous): 공통(또는 소스-동기) 클록 기준으로 셋업/홀드 타이밍을 맞춘다. DDR은 DQS로 데이터 타이밍을 잡는다.
* 비동기(Asynchronous): 요청 신호 간 핸드셰이크(READY/VALID 등)로 동작. SoC 내부 AXI류가 대표적.
* 파이프라이닝·지연 슬롯·커맨드 큐·리오더링으로 처리율을 높인다.

---

## 전기적/물리적 관점

* 임피던스 매칭·온다이 터미네이션(ODT)·라인 종단·스큐(길이 매칭)·크로스타크 억제 등 **신호 무결성(SI)** 이 필수.
* 데이터는 보통 단일 종단(single-ended), 클록/스트로브는 고속에서 차동을 쓰기도 한다.
* 멀티드롭(한 버스에 여러 DIMM)보다 포인트-투-포인트가 고속에 유리.

---

## 메모리와 주변장치의 버스 상호작용

* DMA(Direct Memory Access): 저장장치/네트워크 컨트롤러 등이 CPU 개입 최소화로 메모리에 직접 읽기/쓰기.
* IOMMU: 주변장치의 메모리 접근을 가상화/보호.
* 캐시 일관성: 과거 공유 버스에서는 **버스 스누핑**으로 MESI류 프로토콜을 구현. 현대 멀티코어는 링/메시+디렉터리/스누프 필터로 확장.

---

## GPU·고대역폭 메모리 예시

* GPU “메모리 버스 폭”은 128/192/256/384비트처럼 표기. 폭과 핀당 속도가 대역폭을 좌우한다.
* 예: **256-bit GDDR6 @ 18 Gbps/pin → (18 × 256) / 8 = 576 GB/s**(이론치).
* HBM은 실리콘 인터포저 기반 초광폭(수천 비트)·저클록으로 높은 대역폭과 효율을 얻는다.

---

## 자주 하는 실무 계산과 팁

* DRAM 대역폭(채널 단위): `핀당 전송률(MT/s) × 8 Byte(64비트) = GB/s`

  * DDR4-3200 1채널 = 25.6 GB/s, 2채널 = 51.2 GB/s
  * DDR5-5600 1채널 = 44.8 GB/s, 2채널 = 89.6 GB/s
* 대역폭을 늘리려면: **채널 수 증가**, **데이터 레이트 상승**, **버스 폭 확대(ECC 제외 순수 데이터 폭 기준)**.
* 지연(latency)은 컨트롤러 스케줄링, 행 오픈/클로즈, 프리차지, 뱅크 충돌에 크게 좌우된다.
* ECC를 쓰면 신뢰성은 높아지지만(64+8비트) 유효 데이터 폭/효율이 약간 줄어든다.

---

## 버스와 인터커넥트, 채널을 구분해서 이해하기

* 버스(Bus): 신호선 집합 + 통신 규약(여러 주체가 공유한다는 뉘앙스).
* 인터커넥트(Interconnect/Fabric): 고속 포인트-투-포인트 링크들의 네트워크(링/메시/크로스바).
* 채널(Channel): 메모리 컨트롤러가 병렬로 운용하는 **독립된 메모리 버스 단위**.

---

## 한 문장 요약

메모리 구조에서 버스는 **주소·데이터·제어 신호를 일정한 규약에 따라 전달하는 물리·논리적 통로**이며, 그 성능은 **버스 폭 × 전송률 × 채널 수**와 **컨트롤러 스케줄링/타이밍/신호 무결성**에 의해 결정된다.

